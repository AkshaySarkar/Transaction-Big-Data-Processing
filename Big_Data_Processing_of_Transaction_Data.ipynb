{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkshaySarkar/Transaction-Big-Data-Processing/blob/main/Big_Data_Processing_of_Transaction_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zwFnJsE6vjf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce725f10-3d78-4a70-ff0d-12e474e9b85b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: wget, pyspark\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=84aa76635b02568553f13a40d0e8c98251660f30a48aeb2294e80ff6d35a7cd8\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=965ded23fd00c100c587b01c803aa6d5e51ff9127665b96a2b08c433467b89d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built wget pyspark\n",
            "Installing collected packages: wget, findspark, pyspark\n",
            "Successfully installed findspark-2.0.1 pyspark-3.5.1 wget-3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install wget pyspark  findspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "svV-gAoWdHwR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initializing spark context\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "UT114RfrdRja"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Creating a Spark Session\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark DataFrames basic example\") \\\n",
        "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "JWutIvO7dlXJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading the dataset using wget\n",
        "import pandas as pd\n",
        "import wget\n",
        "\n",
        "link_to_data1 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv'\n",
        "wget.download(link_to_data1)\n",
        "\n",
        "link_to_data2 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv'\n",
        "wget.download(link_to_data2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "leDSgYtTdl8H",
        "outputId": "c564fa15-99cd-4696-9a96-802c4fd7a36b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dataset2.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.read.csv(\"dataset1.csv\", header=True, inferSchema=True)\n",
        "df2 = spark.read.csv(\"dataset2.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "f4I9h4VxBXuP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.printSchema()\n",
        "df2.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpXlIBcfBX7W",
        "outputId": "08f6fb65-6d61-4f8a-fee8-ea5bf706fc93"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- date_column: string (nullable = true)\n",
            " |-- amount: integer (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- transaction_date: string (nullable = true)\n",
            " |-- value: integer (nullable = true)\n",
            " |-- notes: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import year, quarter, to_date\n",
        "\n",
        "#Adding new column 'year' to df1\n",
        "df1 = df1.withColumn('year', year(to_date('date_column','dd/MM/yyyy')))\n",
        "\n",
        "#Adding new column 'quarter' to df2\n",
        "df2 = df2.withColumn('quarter', quarter(to_date('transaction_date','dd/MM/yyyy')))\n",
        "df1.printSchema()\n",
        "df2.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCR-WpvQCa7z",
        "outputId": "0660f657-e36e-41b5-ca9b-d7a6a9fb1801"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- date_column: string (nullable = true)\n",
            " |-- amount: integer (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- transaction_date: string (nullable = true)\n",
            " |-- value: integer (nullable = true)\n",
            " |-- notes: string (nullable = true)\n",
            " |-- quarter: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Renaming df1 column amount to transaction_amount\n",
        "df1 = df1.withColumnRenamed('amount', 'transaction_amount')\n",
        "\n",
        "#Renaming df2 column value to transaction_value\n",
        "df2 = df2.withColumnRenamed('value', 'transaction_value')\n",
        "df1.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okI_49U8Ca_E",
        "outputId": "60eca51b-d19d-4f12-ba3b-2f91071a415e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(customer_id=1, date_column='1/1/2022', transaction_amount=5000, description='Purchase A', location='Store A', year=2022)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping unnecessary columns\n",
        "#Dropping columns description and location from df1\n",
        "df1 = df1.drop('description', 'location')\n",
        "\n",
        "#Drop column notes from df2\n",
        "df2 = df2.drop('notes')\n",
        "df1.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szFV6ousECfm",
        "outputId": "58ea81d4-9f8e-4fb0-dff0-902c10ca9ac9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(customer_id=1, date_column='1/1/2022', transaction_amount=5000, year=2022)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#joining df1 and df2 based on common column customer_id\n",
        "joined_df = df1.join(df2, 'customer_id', 'inner')\n",
        "joined_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3FPbVaVECmM",
        "outputId": "4336b14f-8243-4add-ebab-a8d3172368aa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
            "|customer_id|date_column|transaction_amount|year|transaction_date|transaction_value|quarter|\n",
            "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
            "|          1|   1/1/2022|              5000|2022|        1/1/2022|             1500|      1|\n",
            "|          2|  15/2/2022|              1200|2022|       15/2/2022|             2000|      1|\n",
            "|          3|  20/3/2022|               800|2022|       20/3/2022|             1000|      1|\n",
            "|          4|  10/4/2022|              3000|2022|       10/4/2022|             2500|      2|\n",
            "|          5|   5/5/2022|              6000|2022|        5/5/2022|             1800|      2|\n",
            "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering the dataframe for transaction amount > 1000\n",
        "filtered_df = joined_df.filter(\"transaction_amount > 1000\")\n",
        "filtered_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZTjhDM0FJvN",
        "outputId": "f3405e19-be1e-4020-d9ee-dd51c59b5af8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
            "|customer_id|date_column|transaction_amount|year|transaction_date|transaction_value|quarter|\n",
            "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
            "|          1|   1/1/2022|              5000|2022|        1/1/2022|             1500|      1|\n",
            "|          2|  15/2/2022|              1200|2022|       15/2/2022|             2000|      1|\n",
            "|          4|  10/4/2022|              3000|2022|       10/4/2022|             2500|      2|\n",
            "|          5|   5/5/2022|              6000|2022|        5/5/2022|             1800|      2|\n",
            "|          6|  10/6/2022|              4500|2022|       10/6/2022|             1200|      2|\n",
            "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "\n",
        "# grouping by customer_id and aggregating the sum of transaction amount\n",
        "\n",
        "total_amount_per_customer = filtered_df.groupBy('customer_id').agg(sum('transaction_amount').alias('total_amount'))\n",
        "total_amount_per_customer.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vccBsb90FKDT",
        "outputId": "f0edfcf0-d2bd-4d5b-ce12-574ff7514134"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+\n",
            "|customer_id|total_amount|\n",
            "+-----------+------------+\n",
            "|         31|        3200|\n",
            "|         85|        1800|\n",
            "|         78|        1500|\n",
            "|         34|        1200|\n",
            "|         81|        5500|\n",
            "|         28|        2600|\n",
            "|         76|        2600|\n",
            "|         27|        4200|\n",
            "|         91|        3200|\n",
            "|         22|        1200|\n",
            "+-----------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing total_amount_per_customer to a Hive table named customer_totals\n",
        "total_amount_per_customer.write.mode(\"overwrite\").saveAsTable(\"customer_totals\")"
      ],
      "metadata": {
        "id": "Fn5hmavzFKLn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Writing filtered_df to HDFS in parquet format file filtered_data\n",
        "filtered_df.write.mode(\"overwrite\").parquet(\"filtered_data.parquet\")"
      ],
      "metadata": {
        "id": "JrM3J8GbHwJf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, lit\n",
        "\n",
        "# adding new column with value indicating whether transaction amount is > 5000 or not\n",
        "df1 = df1.withColumn(\"high_value\", when(df1.transaction_amount > 5000, lit(\"Yes\")).otherwise(lit(\"No\")))\n",
        "df1.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4ujYN9qHwYk",
        "outputId": "d905da5d-071c-4c7f-d264-ca42bee6367c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+------------------+----+----------+\n",
            "|customer_id|date_column|transaction_amount|year|high_value|\n",
            "+-----------+-----------+------------------+----+----------+\n",
            "|          1|   1/1/2022|              5000|2022|        No|\n",
            "|          2|  15/2/2022|              1200|2022|        No|\n",
            "|          3|  20/3/2022|               800|2022|        No|\n",
            "|          4|  10/4/2022|              3000|2022|        No|\n",
            "|          5|   5/5/2022|              6000|2022|       Yes|\n",
            "+-----------+-----------+------------------+----+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "#calculating the average transaction value for each quarter in df2\n",
        "average_value_per_quarter = df2.groupBy('quarter').agg(avg(\"transaction_value\").alias(\"avg_trans_val\"))\n",
        "\n",
        "\n",
        "#showing the average transaction value for each quarter in df2\n",
        "average_value_per_quarter.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdfpmd-lqSZA",
        "outputId": "7a1e6261-ef1e-49c6-f001-84c58b0a45ae"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+\n",
            "|quarter|     avg_trans_val|\n",
            "+-------+------------------+\n",
            "|      1| 1111.111111111111|\n",
            "|      3|1958.3333333333333|\n",
            "|      4| 816.6666666666666|\n",
            "|      2|            1072.0|\n",
            "+-------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# writing average_value_per_quarter to a Hive table named quarterly_averages\n",
        "\n",
        "average_value_per_quarter.write.mode(\"overwrite\").saveAsTable(\"quarterly_averages\")"
      ],
      "metadata": {
        "id": "K5AC2rfnqSco"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating the total transaction value for each year in df1 and saving it to hdfs in csv format\n",
        "total_value_per_year = df1.groupBy('year').agg(sum(\"transaction_amount\").alias(\"total_transaction_val\"))\n",
        "total_value_per_year.write.mode(\"overwrite\").csv(\"total_value_per_year.csv\")\n",
        "total_value_per_year.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-Fmnu9LsfTf",
        "outputId": "16a72008-926d-42f4-e4f6-8b3c3557b478"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------------+\n",
            "|year|total_transaction_val|\n",
            "+----+---------------------+\n",
            "|2025|                25700|\n",
            "|2027|                25700|\n",
            "|2023|                28100|\n",
            "|2022|                29800|\n",
            "|2026|                25700|\n",
            "|2029|                25700|\n",
            "|2030|                 9500|\n",
            "|2028|                25700|\n",
            "|2024|                25700|\n",
            "+----+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I___QZaesfWt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}